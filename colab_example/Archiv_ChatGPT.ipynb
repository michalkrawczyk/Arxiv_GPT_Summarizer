{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h7AMvdMkRUW3",
        "OJ3k4aYlKWsa",
        "Yc14x1cEpeA_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.27.6\n",
        "!pip install PyPDF2==3.0.1\n",
        "!pip install arxiv==1.4.7\n",
        "!pip install pyyaml==6.0\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Libraries Installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9oUvo6csph9",
        "outputId": "cea5c474-8a41-48b7-93b7-bdff6c82f519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries Installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Functions"
      ],
      "metadata": {
        "id": "h7AMvdMkRUW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u2d8jyFsW3U"
      },
      "outputs": [],
      "source": [
        "import arxiv \n",
        "import openai\n",
        "from PyPDF2 import PdfReader\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from itertools import islice\n",
        "import json\n",
        "import os\n",
        "from shutil import move\n",
        "from time import sleep\n",
        "from typing import Union, List, Literal, TypedDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('openai_key.yaml', 'r') as f:\n",
        "  # Read API KEY for ChatGPT\n",
        "  openai.api_key = yaml.safe_load(f)[\"openai_api_key\"]\n",
        "\n",
        "assert openai.api_key and openai.api_key != \"YOUR_SECRET_KEY_HERE\", \\\n",
        " \"OpenAI API key not provided - check openai_key.yaml\"\n",
        "\n",
        "with open('prompts.yaml', 'r') as f:\n",
        "  # Read prompts that will be used to make summaries\n",
        "  PROMPTS = yaml.safe_load(f)"
      ],
      "metadata": {
        "id": "yRHUR8FpfcMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt: str, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\" Get Completion from OpenAI model for given prompt\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt: str\n",
        "        Prompt (Instruction) for model\n",
        "    model:\n",
        "        OpenAI model to use\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    String with response for prompt\n",
        "\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "WCcHTnVrP4_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_paper_from_arxiv(id_list: List[str]) -> List[str]:\n",
        "    \"\"\" Download research papers from Arxiv by paper id\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    id_list: List[str]\n",
        "        List with ids for each paper to download\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    papers_downloaded: List[str]\n",
        "        List of paths for each downloaded file\n",
        "\n",
        "    \"\"\"\n",
        "    papers_downloaded = []\n",
        "    search = arxiv.Search(id_list=id_list)\n",
        "    \n",
        "    for paper in tqdm(search.results(), desc=\"Downloading files...\"):\n",
        "        file_path = paper.download_pdf()\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "          papers_downloaded.append(file_path)\n",
        "    \n",
        "    return papers_downloaded"
      ],
      "metadata": {
        "id": "2yDHtsDX0IPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_recent_papers_by_querry(querry: str, limit: float = 10.0) -> List[str]:\n",
        "    \"\"\" Download research papers from Arxiv by result of search querry\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    querry: str\n",
        "        Search querry (e.g. 'Deep Learning')\n",
        "    \n",
        "    limit: float\n",
        "        Maximum number of papers to download.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    papers_downloaded: List[str]\n",
        "        List of paths for each downloaded file\n",
        "\n",
        "    \"\"\"\n",
        "    papers_downloaded = []\n",
        "    search = arxiv.Search(\n",
        "      query = querry,\n",
        "      max_results = limit,\n",
        "      sort_by = arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "    for paper in tqdm(search.results(), desc=\"Downloading files...\"):\n",
        "      file_path = paper.download_pdf()\n",
        "      papers_downloaded.append(file_path)\n",
        "\n",
        "    return papers_downloaded"
      ],
      "metadata": {
        "id": "B9PnaDcp1w1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class PageData:\n",
        "    \"\"\" Container for page(text) data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        Text of the page\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    words_count : int\n",
        "        Number of words in 'text' parameter.\n",
        "        .. note:: Used as aprroximate number of tokens for chatGPT\n",
        "\n",
        "    \"\"\"\n",
        "    text: str\n",
        "\n",
        "    def __add__(self, other_page):\n",
        "        # Create concatenated page with other page or text\n",
        "        if isinstance(other_page, str):\n",
        "            return self.__class__(self.text + ' \\n' + other_page)\n",
        "\n",
        "        return self.__class__(self.text + ' \\n' + other_page.text)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"PageData(number of words: {self.words_count})\"\n",
        "\n",
        "    @property\n",
        "    def words_count(self) -> int:\n",
        "        return len(self.text.split())"
      ],
      "metadata": {
        "id": "OdIlTHT2WUXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class PaperData:\n",
        "    \"\"\" Container for research paper's data (pages with text and filepath to paper)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        Path to PDF file with research paper\n",
        "\n",
        "    \"\"\"\n",
        "    filepath: str\n",
        "    _pages: List[PageData] = field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        reader = PdfReader(self.filepath)\n",
        "        self._pages = [PageData(page.extract_text())\n",
        "                       for page in reader.pages]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Access each page by index\n",
        "        return self._pages[i]\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"PaperData(file: '{self.filepath}', number of pages: {len(self._pages)})\"\n",
        "\n",
        "    def join_pages_by_length(self, max_words: int = 1100) -> List[PageData]:\n",
        "        \"\"\" Concatenate pages by number of maximum words to contain.\n",
        "        \n",
        "        .. note:: Used as token limiter for prompts filling\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_words: int\n",
        "            Maximum number of words allowed per new page, obtained from join\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        joined_pages: List[PageData]\n",
        "            List of pages, after concatenation\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        joined_pages = []\n",
        "        last_join_page = None\n",
        "\n",
        "        for i, page in enumerate(self._pages):\n",
        "            if last_join_page is not None:\n",
        "\n",
        "                if last_join_page.words_count + page.words_count <= max_words:\n",
        "                    # Create joined version of pages\n",
        "                    last_join_page = last_join_page + page\n",
        "\n",
        "                else:\n",
        "                    # Last candidate is too long to add more pages - add it to list and go further\n",
        "                    joined_pages.append(last_join_page)\n",
        "                    last_join_page = page\n",
        "\n",
        "            else:\n",
        "                # last_join_page not exist yet - use current page\n",
        "                last_join_page = page\n",
        "\n",
        "            if i == (len(self._pages) - 1):\n",
        "                # Fill list with last element\n",
        "                joined_pages.append(last_join_page)\n",
        "\n",
        "        return joined_pages\n",
        "      "
      ],
      "metadata": {
        "id": "tBg3Q4KCYdGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(iterable, batch_size: int):\n",
        "    \"\"\" Split iterable into smaller batches\"\"\"\n",
        "    iterator = iter(iterable)\n",
        "    while batch := list(islice(iterator, batch_size)):\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "WfZlOA0C832U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_summary(text: Union[str, PaperData], max_tokens_per_prompt: int = 1200) -> str:\n",
        "    \"\"\" Obtain Summary for research paper\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text: Union[str, PaperData]\n",
        "        String or PaperData object containing text to summarize\n",
        "    max_tokens_per_prompt: int\n",
        "        Limit of words feeded to prompt, to not exceed model's number of maximum tokens.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    response: str\n",
        "        Response from model, containing created summary\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    text_batches = []\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        # Create batches from text, if it's too long\n",
        "        text_batches = [' '.join(word) for word in \\\n",
        "                        create_batches(text.split(' '), max_tokens_per_prompt)]\n",
        "    else:\n",
        "        # Make batches from pages by joining limited with maximum number of words\n",
        "        text_batches = [t.text for t in text.join_pages_by_length(max_tokens_per_prompt)]\n",
        "    \n",
        "    base_prompt = f\"\"\"{PROMPTS[\"summary\"]} ''''{text_batches[0]}'''\"\"\"\n",
        "\n",
        "    #   base_prompt = f\"\"\"\n",
        "    # Identify the following items from given text, delimited by triple backticks:\n",
        "    # - New Features: (listed each of names of new features, functions and functionalities)\n",
        "    # - New Stategies: (listed new stategies and techniques)\n",
        "    # - Problems: (listed tackled problems and approaches)\n",
        "    # - Design: (network design)\n",
        "    # - maximum of three sentences for obtained results. \n",
        "\n",
        "    # Format your response to pointed answears for each category.\n",
        "    # For each feature in identified new features write on the end few sentences of summary\n",
        "\n",
        "    # Review text: '''{text_batches[0]}'''\n",
        "    # \"\"\"\n",
        "\n",
        "    response = get_completion(base_prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "KvysAm0sDKHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summary_3(text: Union[str, PaperData], max_tokens_per_prompt: int = 1200) -> str:\n",
        "    \"\"\" Obtain Summary for research paper\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text: Union[str, PaperData]\n",
        "        String or PaperData object containing text to summarize\n",
        "    max_tokens_per_prompt: int\n",
        "        Limit of words feeded to prompt, to not exceed model's number of maximum tokens.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    response: str\n",
        "        Response from model, containing created summary\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    text_batches = []\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        # Create batches from text, if it's too long\n",
        "        text_batches = [' '.join(word) for word in \\\n",
        "                        create_batches(text.split(' '), max_tokens_per_prompt)]\n",
        "    else:\n",
        "        # Make batches from pages by joining limited with maximum number of words\n",
        "        text_batches = [t.text for t in text.join_pages_by_length(max_tokens_per_prompt)]\n",
        "    \n",
        "    # base_prompt = f\"\"\"{PROMPTS[\"summary\"]} ''''{text_batches[0]}'''\"\"\"\n",
        "\n",
        "    # system_prompt = f\"\"\" Your task is to make summary for research paper. \n",
        "    # You have to identify the following items from given text, delimited by triple backticks:\n",
        "    \n",
        "    # - New Features: (listed each of names of new features, functions and functionalities)\n",
        "    # - New Stategies: (listed new stategies and techniques)\n",
        "    # - Problems: (listed tackled problems and approaches)\n",
        "    # - Design: (network design)\n",
        "    # - maximum of three sentences for obtained results. \n",
        "\n",
        "    # Format your response to pointed answears for each category.\n",
        "    # For each feature in identified new features write on the end few sentences of summary\n",
        "\n",
        "    # With each next text you have to fill your previous response with missing informations\"\"\"\n",
        "\n",
        "\n",
        "    base_prompt = f\"\"\"{PROMPTS[\"summary\"]} ''''{text_batches[0]}'''\"\"\"\n",
        "\n",
        "    responses = [get_completion(base_prompt)]\n",
        "    prompts_used = 1 # Counter for ensuring not exceeding limit rate of GPT (3 prompts / min)\n",
        "\n",
        "    for text_batch in tqdm(text_batches[1:]):\n",
        "      if prompts_used % 4 == 0:\n",
        "        sleep(60)\n",
        "      \n",
        "      continue_prompt = f\"\"\"\n",
        "      You have to identify the following items from given text, delimited by triple backticks:\n",
        "      - New Features: (listed every name for new features, functions and functionalities and corresponding components)\n",
        "      - New Stategies: (listed new stategies and techniques)\n",
        "      - Problems: (listed tackled problems and approaches)\n",
        "      - Design: (network design)\n",
        "      \n",
        "      After that fill the summary in quotes with missing informations.\n",
        "      text: '''{text_batch}'''\n",
        "\n",
        "      summary to fill: {responses[-1]}\n",
        "      \n",
        "      \"\"\"\n",
        "      responses.append(get_completion(base_prompt))\n",
        "      #- New Features: (listed each of names of new features, functions and functionalities)\n",
        "      #       - maximum of three sentences for obtained results. \n",
        "      prompts_used += 1\n",
        "      \n",
        "    return responses"
      ],
      "metadata": {
        "id": "kgw0Lz31mn29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_description_json(text: Union[str, PaperData], max_tokens: int = 1100) -> dict:\n",
        "    \"\"\" Get short description of paper in JSON format\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text: str\n",
        "        Text of the page\n",
        "        .. warning:: If text contain more than 1100 words will be trimmed to that count\n",
        "\n",
        "    max_tokens: int\n",
        "        Maximum number of tokens(words) from paper used in prompt\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    JSON (as dict) with short description of paper\n",
        "\n",
        "    \"\"\"\n",
        "    # fit text into max token ammount\n",
        "    if isinstance(text, str):\n",
        "        # Trim text to fit prompt\n",
        "        text_to_describe = [' '.join(word) for word in text.split(' ')[:max_tokens]]\n",
        "    else:\n",
        "        # Attempt to concatenate pages with maximum length to fit more of them into prompt\n",
        "        text_to_describe = text.join_pages_by_length(max_tokens)[0].text\n",
        "\n",
        "    prompt = f\"\"\"{PROMPTS[\"short_decription_json\"]} '''{text_to_describe}'''\"\"\"\n",
        "\n",
        "    # prompt = f\"\"\"\n",
        "    # Identify the following items from given text, delimited by triple backticks:\n",
        "    # - Model Name\n",
        "    # - Model category(e.g Object Detection, NLP or image generation)\n",
        "    # - SOTA: if Model is State-of-the-Art\n",
        "    # - New Features: new features introduced\n",
        "    # - Year: Year of publish\n",
        "\n",
        "    # Format your response as a JSON object with \\\n",
        "    # \"Model Name\", \"Model Category\", \"SOTA\", \"New Features\" and \"Year\" as the keys.\n",
        "    # If the information isn't present, use \"unknown\" \\\n",
        "    # as the value.\n",
        "    # Make your response as short as possible.\n",
        "    # Format the SOTA value as a boolean.\n",
        "\n",
        "    # Review text: '''{text_to_describe}'''\n",
        "    # \"\"\"\n",
        "\n",
        "    response = get_completion(prompt)\n",
        "\n",
        "    return json.loads(response)\n"
      ],
      "metadata": {
        "id": "dzqmTaohDYSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Files to summary (Choose option)"
      ],
      "metadata": {
        "id": "pvm7IWuyA7Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Specify settings for downloading papers by search querry\n",
        "#@markdown ---\n",
        "#@markdown Note: This will download most recent papers found with querry\n",
        "SEARCH_QUERRY = \"Artificial Inteligence\" #@param {type:\"string\"}\n",
        "assert SEARCH_QUERRY, \"Empty search quarry\"\n",
        "#@markdown ---\n",
        "#@markdown Number of papers to download\n",
        "#@markdown ---\n",
        "\n",
        "NUMBER_OF_PAPERS = 3 #@param {type:\"integer\"}\n",
        "assert NUMBER_OF_PAPERS > 0\n",
        "#@markdown <br>\n",
        "\n",
        "DOWNLOADED_PAPERS = download_recent_papers_by_querry(SEARCH_QUERRY, NUMBER_OF_PAPERS)\n",
        "print(\"\\nDownloaded papers:\")\n",
        "for p in DOWNLOADED_PAPERS:\n",
        "  print(f\"\\t{p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "U4V6CTiRBCJp",
        "outputId": "833b9906-4fea-47be-dc2d-f4dd9300a92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading files...: 3it [00:03,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloaded papers:\n",
            "\t./2305.05666v1.Policy_Gradient_Methods_in_the_Presence_of_Symmetries_and_State_Abstractions.pdf\n",
            "\t./2305.05665v1.ImageBind_One_Embedding_Space_To_Bind_Them_All.pdf\n",
            "\t./2305.05661v1.ShapeCoder_Discovering_Abstractions_for_Visual_Programs_from_Unstructured_Primitives.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>or</b></h3>"
      ],
      "metadata": {
        "id": "RhoJp1NzB-ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Provide papers to download by Arxiv ID\n",
        "#@markdown ---\n",
        "#@markdown Each paper id must be separated by space (' ') or comma (',')\n",
        "PAPERS_BY_ID = \"2301.05586, 2305.04889\" #@param {type:\"string\"}\n",
        "\n",
        "PAPERS_BY_ID = PAPERS_BY_ID.replace(' ', ',').split(',')\n",
        "PAPERS_BY_ID = [p for p in PAPERS_BY_ID if p]\n",
        "DOWNLOADED_PAPERS = download_paper_from_arxiv(PAPERS_BY_ID)\n",
        "print(\"\\nDownloaded papers:\")\n",
        "for p in DOWNLOADED_PAPERS:\n",
        "  print(f\"\\t{p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Wa3SQ7J8B_Pw",
        "outputId": "84d074d2-0fdb-4247-cab9-aa5ab88033f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading files...: 2it [00:01,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloaded papers:\n",
            "\t./2301.05586v1.YOLOv6_v3_0_A_Full_Scale_Reloading.pdf\n",
            "\t./2305.04889v1.Improving_Real_Time_Bidding_in_Online_Advertising_Using_Markov_Decision_Processes_and_Machine_Learning_Techniques.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Examples"
      ],
      "metadata": {
        "id": "IbI0Zee9Redg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single file example usage"
      ],
      "metadata": {
        "id": "OJ3k4aYlKWsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_paper_from_arxiv([\"2301.05586v1\"])"
      ],
      "metadata": {
        "id": "oyFkjNGoKyr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper = PaperData(\"./2301.05586v1.YOLOv6_v3_0_A_Full_Scale_Reloading.pdf\")\n",
        "get_description_json(paper)"
      ],
      "metadata": {
        "id": "DuFiWVu2HT4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper = PaperData(\"./2301.05586v1.YOLOv6_v3_0_A_Full_Scale_Reloading.pdf\")\n",
        "summary = get_summary_3(paper)\n",
        "print(summary[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpQKiQeT15x9",
        "outputId": "5d3b3b0c-02ad-4438-9632-18737a382ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [04:20<00:00, 52.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Features:\n",
            "- YOLOv6 v3.0 has numerous novel enhancements on the network architecture and the training scheme.\n",
            "- The neck of the detector is renewed with a Bi-directional Concatenation (BiC) module to provide more accurate localization signals.\n",
            "- Anchor-aided training (AAT) strategy is proposed to enjoy the advantages of both anchor-based and anchor-free paradigms without touching inference efficiency.\n",
            "- YOLOv6 is deepened to have another stage in the backbone and the neck, which reinforces it to hit a new state-of-the-art performance on the COCO dataset at a high-resolution input.\n",
            "- A new self-distillation strategy is involved to boost the performance of small models of YOLOv6.\n",
            "\n",
            "New Strategies:\n",
            "- Anchor-aided training (AAT) strategy is proposed to enjoy the advantages of both anchor-based and anchor-free paradigms without touching inference efficiency.\n",
            "- A new self-distillation strategy is involved to boost the performance of small models of YOLOv6.\n",
            "\n",
            "Problems:\n",
            "- The YOLO community has been in high spirits since the first two releases.\n",
            "- YOLOv4 reorganized the detection framework into several separate parts (backbone, neck and head), and verified bag-of-freebies and bag-of-specials at the time to design a framework suitable for training on a single GPU.\n",
            "- At present, YOLOv5, YOLOX, PPYOLOE, YOLOv7 and most recently YOLOv8 are all the competing candidates for efficient detectors to deploy.\n",
            "\n",
            "Design:\n",
            "- YOLOv6 has an enhanced-PAN as its detection neck, which is designed to augment localization signals without bringing in excessive computation burden.\n",
            "- A Bi-directional Concatenation (BiC) module is proposed to integrate feature maps of three adjacent layers, which fuses an extra low-level feature from backbone Ci-1 into Pi.\n",
            "- The neck of YOLOv6 is denoted as RepBi-PAN.\n",
            "\n",
            "Results:\n",
            "- YOLOv6-N hits 37.5% AP on the COCO dataset at a throughput of 1187 FPS tested with an NVIDIA Tesla T4 GPU.\n",
            "- YOLOv6-S strikes 45.0% AP at 484 FPS, outperforming other mainstream detectors at the same scale.\n",
            "- YOLOv6-M/L achieve better accuracy performance (50.0%/52.8% respectively) than other detectors at a similar inference speed.\n",
            "- With an extended backbone and neck design, YOLOv6-L6 achieves the state-of-the-art accuracy in real-time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make List with short Summaries"
      ],
      "metadata": {
        "id": "dt6tJLRU_ao1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = []\n",
        "\n",
        "for i, p in enumerate(tqdm(DOWNLOADED_PAPERS)):\n",
        "  try:\n",
        "    if i > 0 and i % 4 == 0:\n",
        "      sleep(60) # Sleep 60 s to not exceed rate of prompts (3 prompts / min)\n",
        "\n",
        "    # make summary for each paper\n",
        "    paper = PaperData(p)\n",
        "    desc = get_description_json(paper)\n",
        "    desc[\"filename\"] = os.path.basename(p)\n",
        "    summaries.append(desc)\n",
        "  \n",
        "  except Exception as err:\n",
        "    print(f\"\"\"Failed to summarize: {p}\n",
        "            - {err}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGgGFCSY8aoy",
        "outputId": "91f92c19-43c2-4d67-8e69-d84ed3b36d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:13<00:00,  4.39s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"short_summary.json\", 'w') as f:\n",
        "  json.dump(summaries, f, indent=2)"
      ],
      "metadata": {
        "id": "mddqtd5RC5T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Packages with longer summary"
      ],
      "metadata": {
        "id": "63IjeiX99pzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import move\n",
        "\n",
        "for i, p in enumerate(tqdm(DOWNLOADED_PAPERS[:1])):\n",
        "  if i > 0 and i % 4 == 0:\n",
        "    sleep(60) # Sleep 60 s to not exceed rate of prompts (3 prompts / min)\n",
        "  \n",
        "  try:\n",
        "    paper = PaperData(p)\n",
        "    summary = get_summary_3(paper)\n",
        "\n",
        "    paper_dir = os.path.splitext(os.path.basename(p))[0].split('.')[-1]\n",
        "    if not os.path.isdir(paper_dir):\n",
        "      os.makedirs(paper_dir)\n",
        "    \n",
        "    move(p, f\"{paper_dir}/{os.path.basename(p)}\")\n",
        "    with open(f\"{paper_dir}/summary.txt\", 'w') as f:\n",
        "      f.write(summary[-1])\n",
        "      \n",
        "  except Exception as err:\n",
        "    print(f\"\"\"Failed to summarize: {p}\n",
        "            - {err}\"\"\")\n"
      ],
      "metadata": {
        "id": "qqA-cWeL9plK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a83e20-6dd4-4133-d766-74a476d3c770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:11<00:45, 11.44s/it]\u001b[A\n",
            " 40%|████      | 2/5 [00:21<00:31, 10.49s/it]\u001b[A\n",
            " 60%|██████    | 3/5 [00:34<00:23, 11.70s/it]\u001b[A\n",
            " 80%|████████  | 4/5 [01:44<00:34, 34.64s/it]\u001b[A\n",
            "100%|██████████| 5/5 [01:57<00:00, 23.54s/it]\n",
            "100%|██████████| 1/1 [02:07<00:00, 127.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in DOWNLOADED_PAPERS:\n",
        "  paper_directory = os.path.splitext(os.path.basename(p))[0].split('.')[-1]\n",
        "  if os.path.isdir(paper_directory):\n",
        "    # Shell commands sometimes don't work with 'f\" '\n",
        "    zipname = paper_directory + \".zip\"\n",
        "    !zip -rm $zipname $paper_directory "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdx-LrzBl_gc",
        "outputId": "1ffbc6a4-5934-4c8e-d424-d1c7da2295b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: YOLOv6_v3_0_A_Full_Scale_Reloading/ (stored 0%)\n",
            "  adding: YOLOv6_v3_0_A_Full_Scale_Reloading/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: YOLOv6_v3_0_A_Full_Scale_Reloading/2301.05586v1.YOLOv6_v3_0_A_Full_Scale_Reloading.pdf (deflated 17%)\n",
            "  adding: YOLOv6_v3_0_A_Full_Scale_Reloading/summary.txt (deflated 56%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Save Paper and summary in Google Drive or download"
      ],
      "metadata": {
        "id": "kIoOvM4RpPQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download"
      ],
      "metadata": {
        "id": "Yc14x1cEpeA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "for p in DOWNLOADED_PAPERS:\n",
        "  zipfile = os.path.splitext(os.path.basename(p))[0].split('.')[-1] + \".zip\"\n",
        "\n",
        "  if os.path.isfile(zipfile):\n",
        "    files.download(zipfile) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CyVV6iuPpnIB",
        "outputId": "6aa1061f-c3db-4fe4-9bbe-7226d85ad4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2c76e7f9-232e-417f-b85c-c4f3e92e1534\", \"YOLOv6_v3_0_A_Full_Scale_Reloading.zip\", 561420)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Move to Google drive"
      ],
      "metadata": {
        "id": "WIhlY_tRqOcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "GOOGLE_DRIVE_OUTPUT_DIRECTORY = \"/content/drive/MyDrive/\"\n",
        "for p in DOWNLOADED_PAPERS:\n",
        "  zipfile = os.path.splitext(os.path.basename(p))[0].split('.')[-1] + \".zip\"\n",
        "\n",
        "  if os.path.isfile(zipfile):\n",
        "    move(zipfile, os.path.join(GOOGLE_DRIVE_OUTPUT_DIRECTORY, zipfile))\n",
        "    print(f\"Moved {zipfile} to {GOOGLE_DRIVE_OUTPUT_DIRECTORY}\")"
      ],
      "metadata": {
        "id": "-xgjIFSTqRH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Download short summary"
      ],
      "metadata": {
        "id": "Hle6_Mbmrnk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"short_summary.json\") "
      ],
      "metadata": {
        "id": "Iz4wGUQCrrRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}